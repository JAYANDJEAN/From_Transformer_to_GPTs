# From_Transformer_to_GPTs

## Introduction

This project covers the following tasks:
1. Understanding and training tokenizers.
2. Implementing Transformers from scratch and using PyTorch.
3. Fine-tuning BERT and GPT models.
4. Training a small version of the LLaMa model in Chinese.
5. Exploring the CodeLLaMA model.
6. Learning and implementing distributed training techniques.

## Tokenizers

1. Training a Byte Pair Encoding (BPE) Tokenizer using the Hugging Face `tokenizers` library.

## Transformers

1. Implementing a Transformer from Scratch
2. Complete German to English translation task with PyTorch's built-in Transformer and the custom Transformer.

## BERT and GPT

1. Fine-Tuning BERT and GPT using the Hugging Face `transformers` library.

## LLaMa

1. Training a Tiny Version of LLaMa-Chinese

## CodeLLaMA

1. Exploring CodeLLaMA

## References
1. [Hugging Face Tokenizers](https://huggingface.co/docs/tokenizers/index)
2. [Hyunwoongko's Transformer](https://github.com/hyunwoongko/transformer)
3. [PyTorch Translation Tutorial](https://pytorch.org/tutorials/beginner/translation_transformer.html)
4. [Harvard NLP Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
5. [Masked language modeling](https://huggingface.co/docs/transformers/tasks/masked_language_modeling)
6. [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling)
5. [PyTorch LLaMa](https://github.com/hkproj/pytorch-llama)
6. [Meta LLaMa](https://github.com/meta-llama/llama)
7. [Baby LLaMa-Chinese](https://github.com/DLLXW/baby-llama2-chinese)
8. [CodeLLaMA GitHub](https://github.com/meta-llama/codellama/tree/main)
